{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc0a188-34d7-4420-becd-402733180d7a",
   "metadata": {},
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n",
    "\n",
    "Answer: The purpose of grid search CV is to systematically explore hyperparameter combinations to find the best model performance. It exhaustively evaluates models with different hyperparameters using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe7f1e-687f-4ca6-b5b5-bf61dfb828c3",
   "metadata": {},
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?\n",
    "\n",
    "Answer: Grid search CV exhaustively searches all specified hyperparameter combinations, while randomized search CV randomly samples a subset. Grid search is suitable for smaller hyperparameter spaces, while randomized search is more efficient for larger spaces or when computational resources are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30148563-8f87-432a-a250-e4a574137f84",
   "metadata": {},
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "\n",
    "Answer: Data leakage refers to when information from the test set unintentionally influences the training process, leading to overestimated model performance. Example: Using future information (e.g., target variable) in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72192cce-5acd-4348-9b47-8c3f701d6087",
   "metadata": {},
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?\n",
    "\n",
    "Answer: To prevent data leakage, it's important to ensure a clear separation between training and test data, use proper cross-validation techniques, and avoid using future information or data that is not available during model deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3199a1-d40c-4de1-b123-182e7a1fced7",
   "metadata": {},
   "source": [
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "\n",
    "Answer: A confusion matrix is a table that summarizes the performance of a classification model by showing the counts of true positive, true negative, false positive, and false negative predictions, providing insights into the model's accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f108cf-a194-4d86-be33-9bce14dae69e",
   "metadata": {},
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix?\n",
    "\n",
    "Answer: Precision measures the proportion of true positive predictions out of all positive predictions, while recall measures the proportion of true positive predictions out of all actual positive instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf4b4f2-f7a4-449d-b125-c350f3a20cee",
   "metadata": {},
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making in 25 words\n",
    "\n",
    "Answer: By analyzing the confusion matrix, you can identify which types of errors your model is making: false positives (Type I errors) and false negatives (Type II errors), providing insights into the specific misclassifications and potential areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e50469-0725-477e-871b-018240d1ccb3",
   "metadata": {},
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated.\n",
    "\n",
    "Answer: Common metrics derived from a confusion matrix include accuracy (TP+TN/Total), precision (TP/TP+FP), recall (TP/TP+FN), and F1-score (2*(precision*recall)/(precision+recall))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28305471-9860-4b3f-a5b9-fa63570a84f3",
   "metadata": {},
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix in 25 words\n",
    "\n",
    "Answer: The accuracy of a model is calculated using the values in the confusion matrix (TP, TN, FP, FN) and represents the proportion of correct predictions (TP+TN) out of the total predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a6f1f8-bf64-467d-a19b-62a25a1245f8",
   "metadata": {},
   "source": [
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model\n",
    "\n",
    "Answer: By examining the confusion matrix, you can identify if the model exhibits imbalanced predictions across classes, indicating potential biases or limitations in handling certain categories of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191993d1-bdd2-40c9-a50a-71348afc32c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
